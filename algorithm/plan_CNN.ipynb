{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes of variables\n",
    "\n",
    "1. Placeholders:\n",
    "   - X: (None, width, height, num_channel)\n",
    "   - Y: (None, num_classes)\n",
    "<br>\n",
    "<br>\n",
    "2. Input data:\n",
    "   - train_x: (17..., width, height)\n",
    "   - train_y: (17..., num_classes)\n",
    "<br>\n",
    "<br>\n",
    "3. Batches:\n",
    "    - batch_x: (batch_size, width, height)\n",
    "    - batch_y: (batch_size, num_classes)\n",
    "    - batch_prediction: (batch_size, num_classes)\n",
    "<br>\n",
    "<br>\n",
    "4. Logists: \n",
    "    - The output of the model with shape: (batch_size, num_classes)<br>\n",
    "    This value will be compared with the input of placeholder Y and used to calculate the loss thsi model\n",
    "\n",
    "\n",
    "\n",
    "## Problems occurred\n",
    "\n",
    "1. The mismatch when using [cross_entropy](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) as loss function:\n",
    "    The rank of logists minus the rank of targets should be 1. <br>\n",
    "    e.g. logists: (batch_size, num_class) hold the probability while targets: (batch_size) hold the real label only\n",
    "2. The number of channel if use CNN. The original data has no dimension of number of channel\n",
    "3. When use get_acc function, also the rank problem\n",
    "   ```python\n",
    "   bingo = np.sum(np.equal(batch_predictions, labels))\n",
    "   has error:\n",
    "   operands could not be broadcast together with shapes (500,) (500,13)\n",
    "   ```\n",
    "4. The training time is so long may be it is necessaray to combine the network with autoencoder. Try variational autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\trainingData.csv\",header = 0)\n",
    "features = scale(np.asarray(dataset.ix[:,0:520]))\n",
    "\n",
    "# --------------------- Training and validation set --------------- #\n",
    "\n",
    "# Convert RSS vector into 26*20 matrix\n",
    "maps = []\n",
    "for i in range (features.shape[0]):\n",
    "    temp_map = np.reshape(features[i,:], (26, 20, 1))\n",
    "    maps.append(temp_map)\n",
    "maps = np.asarray(maps)\n",
    "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
    "labels = np.asarray(pd.get_dummies(labels))\n",
    "\n",
    "print('labels shape:', labels.shape)\n",
    "print(maps.shape)\n",
    "\n",
    "train_val_split = np.random.rand(len(features)) < 0.90\n",
    "train_x = maps[train_val_split]\n",
    "train_y = labels[train_val_split]\n",
    "val_x = maps[~train_val_split]\n",
    "val_y = labels[~train_val_split]\n",
    "\n",
    "# --------------------- Testing set --------------- #\n",
    "\n",
    "test_dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\validationData.csv\",header = 0)\n",
    "test_features = scale(np.asarray(test_dataset.ix[:,0:520]))\n",
    "# Convert RSS vector into 26*20 matrix\n",
    "test_maps = []\n",
    "for i in range (test_features.shape[1]):\n",
    "    temp_map = np.reshape(test_features[i,:], (26, 20, 1))\n",
    "    test_maps.append(temp_map)\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))\n",
    "\n",
    "# --------------------- Def functions for variable declaration --------------- #\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0, shape = shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# --------------------- Model parameters --------------- #\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 500\n",
    "map_width = train_x[0].shape[0]\n",
    "map_height = train_x[0].shape[1]\n",
    "num_channels = 1\n",
    "\n",
    "n_input = 520 \n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 128 \n",
    "n_hidden_3 = 64 \n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "conv_kernel = 4\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2 # NxN window for 1st max pool layer\n",
    "max_pool_size2 = 2 # NxN window for 2nd max pool layer\n",
    "fully_connected_size1 = 100\n",
    "keep_prob = 1\n",
    "\n",
    "total_batches = dataset.shape[0] // batch_size\n",
    "\n",
    "# --------------------- Placeholders --------------- #\n",
    "\n",
    "x_input_shape = (None, map_width, map_height, num_channels)\n",
    "X = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "Y = tf.placeholder(tf.int32, shape = (None, n_classes))\n",
    "\n",
    "# --------------------- Initailize model --------------- #\n",
    "def my_conv_net(input_data):\n",
    "    # 1st layer: 100C3-MP2\n",
    "    conv_1 = slim.conv2d(input_data, 100, [3, 3], 1, padding='SAME', scope='conv1',activation_fn=tf.nn.relu)\n",
    "    max_pool1 = slim.max_pool2d(conv_1, [2, 2], [2, 2], padding='SAME')\n",
    "\n",
    "    # 2nd layer: 200C2-MP2\n",
    "    conv_2 = slim.conv2d(max_pool1, 200, [2, 2], 1, padding='SAME', scope='conv2',activation_fn=tf.nn.relu)\n",
    "    max_pool2 = max_pool_1 = slim.max_pool2d(conv_2, [2, 2], [2, 2], padding='SAME')\n",
    "\n",
    "    # Flat the output from conv layers for next fully connected layers\n",
    "    flatten = slim.flatten(max_pool2)\n",
    "    \n",
    "    # 1st fully connected layer\n",
    "    fc1 = slim.fully_connected(slim.dropout(flatten, keep_prob), 1024,\n",
    "                                   activation_fn=tf.nn.tanh, scope='fc1')\n",
    "\n",
    "    # 2nd fully connected layer\n",
    "    model_output = slim.fully_connected(slim.dropout(fc1, keep_prob), n_classes,\n",
    "                                   activation_fn=None, scope='fc2')\n",
    "\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "model_output = my_conv_net(X)\n",
    "\n",
    "# Declare Loss Function (softmax cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=tf.reduce_max(Y,1)))\n",
    "\n",
    "# Create a prediction function\n",
    "prediction = tf.nn.softmax(model_output)\n",
    "\n",
    "# Create an optimizer\n",
    "my_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "# Calculate accuracy function\n",
    "# In this function, batch_prediction is the ouput result from the CNN\n",
    "# while labels are the real label stored in dataset which trains the model\n",
    "def get_acc(logists, labels):\n",
    "    batch_predictions = np.argmax(logists, axis=1)\n",
    "    bingo = np.sum(np.equal(batch_predictions, np.max(labels, 1)))\n",
    "    return(100. * bingo/batch_predictions.shape[0])\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_loss = np.empty(0)\n",
    "        epoch_acc = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "\n",
    "            sess.run(train_step, feed_dict={X: batch_x, Y : batch_y})\n",
    "            batch_prediction, batch_loss = sess.run([prediction, loss], feed_dict={X: batch_x, Y : batch_y})\n",
    "            batch_acc = get_acc(batch_prediction, batch_y)\n",
    "            print(\"!!!!!\")\n",
    "            epoch_loss = np.append(epoch_loss, batch_loss)\n",
    "            print(\"!!!!!\")\n",
    "            epoch_acc = np.append(epoch_acc, batch_acc)\n",
    "            print(b)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_loss),\" Training Accuracy: \", \\\n",
    "            sess.run(np.mean(epoch_acc), feed_dict={X: train_x, Y: train_y}), \\\n",
    "            \"Validation Accuracy:\", sess.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
    "    print (\"Supervised training finished...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
