{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders and Neural Network for Place recognition with WiFi fingerprints\n",
    "Implementation of algorithm discussed in <a href=\"https://arxiv.org/pdf/1611.02049v1.pdf\">Low-effort place recognition with WiFi fingerprints using Deep Learning </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\trainingData.csv\",header = 0)\n",
    "features = scale(np.asarray(dataset.ix[:,0:520]))\n",
    "# features = normalize(np.asarray(dataset.ix[:,0:520]))\n",
    "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
    "labels = np.asarray(pd.get_dummies(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing UJIndoorLoc training data set into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(features)) < 0.90\n",
    "train_x = features[train_val_split]\n",
    "train_y = labels[train_val_split]\n",
    "val_x = features[~train_val_split]\n",
    "val_y = labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using UJIndoorLoc validation data set as testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\validationData.csv\",header = 0)\n",
    "test_features = scale(np.asarray(test_dataset.ix[:,0:520]))\n",
    "# test_features = normalize(np.asarray(test_dataset.ix[:,0:520]))\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 520 \n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 128 \n",
    "n_hidden_3 = 64 \n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "total_batches = dataset.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
    "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "\n",
    "# --------------------- Encoder Variables --------------- #\n",
    "\n",
    "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
    "e_biases_h1 = bias_variable([n_hidden_1])\n",
    "\n",
    "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
    "e_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "e_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "# --------------------- Decoder Variables --------------- #\n",
    "\n",
    "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "d_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
    "d_biases_h2 = bias_variable([n_hidden_1])\n",
    "\n",
    "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
    "d_biases_h3 = bias_variable([n_input])\n",
    "\n",
    "# --------------------- DNN Variables ------------------ #\n",
    "\n",
    "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
    "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "#dnn_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "#dnn_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
    "dnn_biases_out = bias_variable([n_classes])\n",
    "\n",
    "# --------------------- CNN Variables ------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
    "    return l3\n",
    "    \n",
    "def decode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
    "    return l3\n",
    "\n",
    "def dnn(x):\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
    "    #dropout = tf.nn.dropout(l1, 0.5)\n",
    "    l2 = tf.nn.relu(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
    "    #l3 = tf.nn.relu(tf.add(tf.matmul(l2,dnn_weights_h3),dnn_biases_h3))\n",
    "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded = encode(X)\n",
    "decoded = decode(encoded) \n",
    "y_ = dnn(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of optimizer\n",
    "\n",
    "The following block of code is for the **AutoEncoder**. Calculate the result of **loss function** or **cost function** to evaluate autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
    "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
    "us_optimizer = tf.train.AdamOptimizer(learning_rate/10).minimize(us_cost_function)\n",
    "s_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(s_cost_function)\n",
    "# GradientDescent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy\n",
    "Following 2 lines is compare the results predicted by the network and the data of location in the testing set. `shape(y_) = [1111, 13]` while `shape(Y) = [1111]`as the there a 1111 testing data and the number of location lables is 13. The ocntents in `y_`, for each row, is the **possibility** that the point locates at the labled position. \n",
    "<br>\n",
    "<br>\n",
    "Function `tf.argmax(input, axis = 1)` will returns the **index** with the largest value across axes of a tensor but **not** the exact **value** of the maximum. If the shape of `input` is [12, 3], `axis = 1` will make the function takes index of maximum for each line, which means the shape of result will be [12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "Image take from: https://arxiv.org/pdf/1611.02049v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AE.png\">\n",
    "<img src=\"NN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-b1379d6f5ac3>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-b1379d6f5ac3>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \",             session.run(accuracy, feed_dict={X: train_x, Y: train_y}),             \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))s\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (features.shape[0] - batch_size)\n",
    "            batch_x = features[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
    "    print (\"Unsupervised pre-training finished...\")\n",
    "    \n",
    "    \n",
    "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
    "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
    "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
    "            \n",
    "    print (\"Supervised training finished...\")\n",
    "\n",
    "    print (\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))\n",
    "    \n",
    "    print(session.run(y_, feed_dict={X: test_features, Y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Record\n",
    "\n",
    "2017/6/28 <br>\n",
    "- The output of the network is a [n_input, n_classes] tensor, each row represents the posibility of lables for each RSS.\n",
    "- Modify the number of training samples (reducing the scale of validation set). But the original rate 0.7 seems appropriate. Either higher or lower will reduce the accuracy.\n",
    "\n",
    "2017/7/5 <br>\n",
    "- Change the activation function in DNN classifier from `tf.nn.tanh` to `tf.nn.relu`. This just slightly improve the accuracy. If change all activation function even in autoencoder, the result even worse.\n",
    "\n",
    "2017/7/11 <br>\n",
    "- Try to add a hidden layer with 64 neural cells in DNN classifier. The result has no remarkable improvement. \n",
    "- Try to change the optimizer 'Adam' as same as the keras sample, the result even worse.\n",
    "\n",
    "2017/7/12 <br>\n",
    "- Try to use 'Adam' Optimizer with `learning_rate=1`, it is a little bit faster but, when `epoch = 20`, something wrong with training of DNN part. Once the epoch exceeds 13, it seems overfitting.\n",
    "\n",
    "\n",
    "2017/7/16 <br>\n",
    "Some suggestions from Jaehoo:\n",
    "- co-validation...\n",
    "- leaky relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
