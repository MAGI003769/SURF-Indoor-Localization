{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders and Neural Network for Place recognition with WiFi fingerprints\n",
    "Implementation of algorithm discussed in <a href=\"https://arxiv.org/pdf/1611.02049v1.pdf\">Low-effort place recognition with WiFi fingerprints using Deep Learning </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\trainingData.csv\",header = 0)\n",
    "features = scale(np.asarray(dataset.ix[:,0:520]))\n",
    "# features = normalize(np.asarray(dataset.ix[:,0:520]))\n",
    "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
    "labels = np.asarray(pd.get_dummies(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing UJIndoorLoc training data set into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(features)) < 0.90\n",
    "train_x = features[train_val_split]\n",
    "train_y = labels[train_val_split]\n",
    "val_x = features[~train_val_split]\n",
    "val_y = labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using UJIndoorLoc validation data set as testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\validationData.csv\",header = 0)\n",
    "test_features = scale(np.asarray(test_dataset.ix[:,0:520]))\n",
    "# test_features = normalize(np.asarray(test_dataset.ix[:,0:520]))\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 520 \n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 128 \n",
    "n_hidden_3 = 64 \n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "total_batches = dataset.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
    "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "\n",
    "# --------------------- Encoder Variables --------------- #\n",
    "\n",
    "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
    "e_biases_h1 = bias_variable([n_hidden_1])\n",
    "\n",
    "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
    "e_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "e_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "# --------------------- Decoder Variables --------------- #\n",
    "\n",
    "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "d_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
    "d_biases_h2 = bias_variable([n_hidden_1])\n",
    "\n",
    "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
    "d_biases_h3 = bias_variable([n_input])\n",
    "\n",
    "# --------------------- DNN Variables ------------------ #\n",
    "\n",
    "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
    "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "#dnn_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "#dnn_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
    "dnn_biases_out = bias_variable([n_classes])\n",
    "\n",
    "# --------------------- CNN Variables ------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
    "    return l3\n",
    "    \n",
    "def decode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
    "    return l3\n",
    "\n",
    "def dnn(x):\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
    "    #dropout = tf.nn.dropout(l1, 0.5)\n",
    "    l2 = tf.nn.relu(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
    "    #l3 = tf.nn.relu(tf.add(tf.matmul(l2,dnn_weights_h3),dnn_biases_h3))\n",
    "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode(X)\n",
    "decoded = decode(encoded) \n",
    "y_ = dnn(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of optimizer\n",
    "\n",
    "The following block of code is for the **AutoEncoder**. Calculate the result of **loss function** or **cost function** to evaluate autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
    "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
    "us_optimizer = tf.train.AdamOptimizer(learning_rate/10).minimize(us_cost_function)\n",
    "s_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(s_cost_function)\n",
    "# GradientDescent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy\n",
    "Following 2 lines is compare the results predicted by the network and the data of location in the testing set. `shape(y_) = [1111, 13]` while `shape(Y) = [1111]`as the there a 1111 testing data and the number of location lables is 13. The ocntents in `y_`, for each row, is the **possibility** that the point locates at the labled position. \n",
    "<br>\n",
    "<br>\n",
    "Function `tf.argmax(input, axis = 1)` will returns the **index** with the largest value across axes of a tensor but **not** the exact **value** of the maximum. If the shape of `input` is [12, 3], `axis = 1` will make the function takes index of maximum for each line, which means the shape of result will be [12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "Image take from: https://arxiv.org/pdf/1611.02049v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AE.png\">\n",
    "<img src=\"NN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.729340485911\n",
      "Epoch:  1  Loss:  0.708988087576\n",
      "Epoch:  2  Loss:  0.701351931189\n",
      "Epoch:  3  Loss:  0.697761710918\n",
      "Epoch:  4  Loss:  0.695179305948\n",
      "Epoch:  5  Loss:  0.69250822591\n",
      "Epoch:  6  Loss:  0.69061423692\n",
      "Epoch:  7  Loss:  0.689562432995\n",
      "Epoch:  8  Loss:  0.687908857863\n",
      "Epoch:  9  Loss:  0.686813461275\n",
      "Epoch:  10  Loss:  0.686273877372\n",
      "Epoch:  11  Loss:  0.685350349742\n",
      "Epoch:  12  Loss:  0.684753693476\n",
      "Epoch:  13  Loss:  0.6842446377\n",
      "Epoch:  14  Loss:  0.683298305559\n",
      "Epoch:  15  Loss:  0.683183146485\n",
      "Epoch:  16  Loss:  0.682448952422\n",
      "Epoch:  17  Loss:  0.681667572288\n",
      "Epoch:  18  Loss:  0.681646542488\n",
      "Epoch:  19  Loss:  0.681233391917\n",
      "Unsupervised pre-training finished...\n",
      "Epoch:  0  Loss:  3.47160111668  Training Accuracy:  0.518469 Validation Accuracy: 0.51006\n",
      "Epoch:  1  Loss:  1.33416180687  Training Accuracy:  0.670845 Validation Accuracy: 0.657445\n",
      "Epoch:  2  Loss:  0.798248211751  Training Accuracy:  0.76021 Validation Accuracy: 0.741449\n",
      "Epoch:  3  Loss:  0.5437905743  Training Accuracy:  0.800435 Validation Accuracy: 0.788732\n",
      "Epoch:  4  Loss:  0.397072644085  Training Accuracy:  0.832135 Validation Accuracy: 0.809356\n",
      "Epoch:  5  Loss:  0.309757846073  Training Accuracy:  0.856371 Validation Accuracy: 0.836519\n",
      "Epoch:  6  Loss:  0.258189821602  Training Accuracy:  0.873698 Validation Accuracy: 0.849598\n",
      "Epoch:  7  Loss:  0.22894468307  Training Accuracy:  0.886233 Validation Accuracy: 0.862676\n",
      "Epoch:  8  Loss:  0.197165771183  Training Accuracy:  0.903449 Validation Accuracy: 0.885312\n",
      "Epoch:  9  Loss:  0.174585734805  Training Accuracy:  0.904173 Validation Accuracy: 0.882294\n",
      "Epoch:  10  Loss:  0.162123544965  Training Accuracy:  0.934258 Validation Accuracy: 0.911469\n",
      "Epoch:  11  Loss:  0.153397348264  Training Accuracy:  0.939551 Validation Accuracy: 0.913481\n",
      "Epoch:  12  Loss:  0.141432782699  Training Accuracy:  0.944844 Validation Accuracy: 0.915996\n",
      "Epoch:  13  Loss:  0.132637584252  Training Accuracy:  0.951752 Validation Accuracy: 0.926056\n",
      "Epoch:  14  Loss:  0.125298961773  Training Accuracy:  0.953312 Validation Accuracy: 0.925553\n",
      "Epoch:  15  Loss:  0.120802132278  Training Accuracy:  0.956042 Validation Accuracy: 0.931087\n",
      "Epoch:  16  Loss:  0.117515166761  Training Accuracy:  0.959719 Validation Accuracy: 0.933602\n",
      "Epoch:  17  Loss:  0.111939917039  Training Accuracy:  0.964733 Validation Accuracy: 0.935111\n",
      "Epoch:  18  Loss:  0.109459292653  Training Accuracy:  0.971363 Validation Accuracy: 0.942153\n",
      "Epoch:  19  Loss:  0.104750812957  Training Accuracy:  0.97348 Validation Accuracy: 0.943662\n",
      "Supervised training finished...\n",
      "\n",
      "Testing Accuracy: 0.717372\n",
      "[[  3.63997649e-03   9.41125109e-05   1.01135911e-05 ...,   7.46389735e-04\n",
      "    7.87156932e-06   2.48007946e-05]\n",
      " [  3.04688399e-07   4.14391715e-10   3.78708513e-08 ...,   3.72951492e-09\n",
      "    1.72095359e-04   9.84564543e-01]\n",
      " [  3.56564271e-08   1.23321698e-12   2.07903850e-09 ...,   9.59971685e-11\n",
      "    3.62271919e-08   9.99999404e-01]\n",
      " ..., \n",
      " [  9.89674628e-01   4.03387472e-04   3.67979460e-08 ...,   1.90666071e-07\n",
      "    6.22527523e-08   2.49990239e-06]\n",
      " [  9.85686123e-01   9.02081811e-05   3.40326345e-09 ...,   3.76271245e-08\n",
      "    1.22719266e-08   3.45728449e-06]\n",
      " [  9.99695539e-01   1.63503079e-04   2.06512940e-09 ...,   4.63018379e-09\n",
      "    3.66536246e-09   2.57488665e-07]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (features.shape[0] - batch_size)\n",
    "            batch_x = features[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
    "    print (\"Unsupervised pre-training finished...\")\n",
    "    \n",
    "    \n",
    "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
    "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
    "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
    "            \n",
    "    print (\"Supervised training finished...\")\n",
    "    \n",
    "\n",
    "    print (\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))\n",
    "    \n",
    "    print(session.run(y_, feed_dict={X: test_features, Y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Record\n",
    "\n",
    "2017/6/28 <br>\n",
    "- The output of the network is a [n_input, n_classes] tensor, each row represents the posibility of lables for each RSS.\n",
    "- Modify the number of training samples (reducing the scale of validation set). But the original rate 0.7 seems appropriate. Either higher or lower will reduce the accuracy.\n",
    "\n",
    "2017/7/5 <br>\n",
    "- Change the activation function in DNN classifier from `tf.nn.tanh` to `tf.nn.relu`. This just slightly improve the accuracy. If change all activation function even in autoencoder, the result even worse.\n",
    "\n",
    "2017/7/11 <br>\n",
    "- Try to add a hidden layer with 64 neural cells in DNN classifier. The result has no remarkable improvement. \n",
    "- Try to change the optimizer 'Adam' as same as the keras sample, the result even worse.\n",
    "\n",
    "2017/7/12 <br>\n",
    "- Try to use 'Adam' Optimizer with `learning_rate=1`, it is a little bit faster but, when `epoch = 20`, something wrong with training of DNN part. Once the epoch exceeds 13, it seems overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
