{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_train = \"UJIndoorLoc/trainingData.csv\"\n",
    "path_validation = \"UJIndoorLoc/validationData.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Explicitly pass header=0 to be able to replace existing names \n",
    "train_df = pd.read_csv(path_train,header = 0)\n",
    "train_df = train_df[:19930]\n",
    "train_AP_strengths = train_df.ix[:,:520] #select first 520 columns\n",
    "\n",
    "#Scale transforms data to center to the mean and component wise scale to unit variance\n",
    "train_AP_features = scale(np.asarray(train_AP_strengths))\n",
    "\n",
    "#The following two objects are actually pandas.core.series.Series objects\n",
    "building_ids_str = train_df[\"BUILDINGID\"].map(str) #convert all the building ids to strings\n",
    "building_floors_str = train_df[\"FLOOR\"].map(str) #convert all the building floors to strings\n",
    "\n",
    "res = building_ids_str + building_floors_str #element wise concatenation of BUILDINGID+FLOOR\n",
    "train_labels = np.asarray(building_ids_str + building_floors_str)\n",
    "\n",
    "#convert labels to categorical variables, dummy_labels has type 'pandas.core.frame.DataFrame'\n",
    "dummy_labels = pd.get_dummies(train_labels)\n",
    "\n",
    "\n",
    "\"\"\"one hot encode the dummy_labels.\n",
    "this is done because dummy_labels is a dataframe with the labels (BUILDINGID+FLOOR) \n",
    "as the column names\n",
    "\"\"\"\n",
    "train_labels = np.asarray(dummy_labels) #labels is an array of shape 19937 x 13. (there are 13 types of labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate len(train_AP_features) of floats in between 0 and 1\n",
    "train_val_split = np.random.rand(len(train_AP_features))\n",
    "#convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "train_val_split = train_val_split < 0.90 #should contain ~70% percent true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't given a formal testing set, so we will treat the given validation set as the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will then split our given training set into training + validation \n",
    "train_X = train_AP_features[train_val_split]\n",
    "train_y = train_labels[train_val_split]\n",
    "val_X = train_AP_features[~train_val_split]\n",
    "val_y = train_labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Turn the given validation set into a testing set\n",
    "test_df = pd.read_csv(path_validation,header = 0)\n",
    "test_AP_features = scale(np.asarray(test_df.ix[:,0:520]))\n",
    "test_labels = np.asarray(test_df[\"BUILDINGID\"].map(str) + test_df[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epochs = 20\n",
    "batch_size = 32\n",
    "input_size = 520\n",
    "num_classes = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=input_size, activation='tanh', bias=True))\n",
    "    model.add(Dense(128, activation='tanh', bias=True))\n",
    "    model.add(Dense(64, activation='tanh', bias=True))\n",
    "    model.add(Dense(32, activation='tanh', bias=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(e):   \n",
    "    e.add(Dense(64, input_dim=32, activation='tanh', bias=True))\n",
    "    e.add(Dense(128, activation='tanh', bias=True))\n",
    "    e.add(Dense(256, activation='tanh', bias=True))\n",
    "    e.add(Dense(input_size, activation='tanh', bias=True))\n",
    "    e.compile(optimizer='adam', loss='mse')\n",
    "    return e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, use_bias=True, activation=\"tanh\", input_dim=520)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, use_bias=True, activation=\"tanh\")`\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, use_bias=True, activation=\"tanh\")`\n",
      "  \"\"\"\n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, use_bias=True, activation=\"tanh\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "e = encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, use_bias=True, activation=\"tanh\", input_dim=32)`\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, use_bias=True, activation=\"tanh\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, use_bias=True, activation=\"tanh\")`\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(520, use_bias=True, activation=\"tanh\")`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "d = decoder(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "17992/17992 [==============================] - ETA: 0s - loss: 0.730 - 3s - loss: 0.7291     \n",
      "Epoch 2/15\n",
      "17992/17992 [==============================] - 2s - loss: 0.7052     \n",
      "Epoch 3/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6989     \n",
      "Epoch 4/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6944     \n",
      "Epoch 5/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6909     \n",
      "Epoch 6/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6879     \n",
      "Epoch 7/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6852     \n",
      "Epoch 8/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6831     \n",
      "Epoch 9/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6811     \n",
      "Epoch 10/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6795     \n",
      "Epoch 11/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6780     \n",
      "Epoch 12/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6765     \n",
      "Epoch 13/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6753     \n",
      "Epoch 14/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6743     \n",
      "Epoch 15/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.6732     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2075107cc88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.fit(train_X, train_X, epochs=nb_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(d):\n",
    "    d.add(Dense(128, input_dim=input_size, activation='relu', bias=True))\n",
    "    #num_to_remove = 3\n",
    "    #for i in range(num_to_remove):\n",
    "        #d.pop()\n",
    "    d.add(Dropout(0.75))\n",
    "    d.add(Dense(128, activation='relu', bias=True))\n",
    "    #d.add(Dropout(0.75))\n",
    "    #d.add(Dense(32, activation='relu', bias=True))\n",
    "    d.add(Dense(num_classes, activation='softmax', bias=True))\n",
    "    d.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, use_bias=True, activation=\"relu\", input_dim=520)`\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, use_bias=True, activation=\"relu\")`\n",
      "  import sys\n",
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(13, use_bias=True, activation=\"softmax\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "c = classifier(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17992 samples, validate on 1938 samples\n",
      "Epoch 1/15\n",
      "17992/17992 [==============================] - 4s - loss: 0.6969 - acc: 0.7549 - val_loss: 0.2343 - val_acc: 0.9319\n",
      "Epoch 2/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.3409 - acc: 0.8936 - val_loss: 0.1925 - val_acc: 0.9505\n",
      "Epoch 3/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.2674 - acc: 0.9230 - val_loss: 0.2621 - val_acc: 0.9376\n",
      "Epoch 4/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.2408 - acc: 0.9324 - val_loss: 0.1934 - val_acc: 0.9469\n",
      "Epoch 5/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.2044 - acc: 0.9415 - val_loss: 0.2132 - val_acc: 0.9479\n",
      "Epoch 6/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.2011 - acc: 0.9463 - val_loss: 0.1555 - val_acc: 0.9639\n",
      "Epoch 7/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1579 - acc: 0.9547 - val_loss: 0.2078 - val_acc: 0.9432\n",
      "Epoch 8/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1577 - acc: 0.9581 - val_loss: 0.1859 - val_acc: 0.9551\n",
      "Epoch 9/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1338 - acc: 0.9628 - val_loss: 0.1747 - val_acc: 0.9499\n",
      "Epoch 10/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1535 - acc: 0.9573 - val_loss: 0.1970 - val_acc: 0.9536\n",
      "Epoch 11/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1415 - acc: 0.9615 - val_loss: 0.1668 - val_acc: 0.9572\n",
      "Epoch 12/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1318 - acc: 0.9653 - val_loss: 0.1487 - val_acc: 0.9634\n",
      "Epoch 13/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1186 - acc: 0.9667 - val_loss: 0.1578 - val_acc: 0.9639\n",
      "Epoch 14/15\n",
      "17992/17992 [==============================] - 4s - loss: 0.1045 - acc: 0.9702 - val_loss: 0.1835 - val_acc: 0.9505\n",
      "Epoch 15/15\n",
      "17992/17992 [==============================] - 3s - loss: 0.1259 - acc: 0.9657 - val_loss: 0.1575 - val_acc: 0.9536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207538dae80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.fit(train_X, train_y, validation_data=(val_X, val_y), epochs=nb_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 800/1111 [====================>.........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "loss, acc = c.evaluate(test_AP_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5294207744 0.725472547255\n"
     ]
    }
   ],
   "source": [
    "print (loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- This program has a little bit higher accuracy, around 3%, compared with the other sample. I don't think it has a larger \n",
    "- The autoencoder includes a encode part and a decode part. The dimension of output should be as same as the dimension of input. But in original code, the DNN classifier has its first hidden layer with input dimension with 64 not 520 which is the input size. Is it directly connected with only encode part?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
