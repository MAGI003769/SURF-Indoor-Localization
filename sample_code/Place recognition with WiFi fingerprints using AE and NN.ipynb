{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders and Neural Network for Place recognition with WiFi fingerprints\n",
    "Implementation of algorithm discussed in <a href=\"https://arxiv.org/pdf/1611.02049v1.pdf\">Low-effort place recognition with WiFi fingerprints using Deep Learning </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\trainingData.csv\",header = 0)\n",
    "features = scale(np.asarray(dataset.ix[:,0:520]))\n",
    "# features = normalize(np.asarray(dataset.ix[:,0:520]))\n",
    "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
    "labels = np.asarray(pd.get_dummies(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing UJIndoorLoc training data set into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(features)) < 0.90\n",
    "train_x = features[train_val_split]\n",
    "train_y = labels[train_val_split]\n",
    "val_x = features[~train_val_split]\n",
    "val_y = labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using UJIndoorLoc validation data set as testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\validationData.csv\",header = 0)\n",
    "test_features = scale(np.asarray(test_dataset.ix[:,0:520]))\n",
    "# test_features = normalize(np.asarray(test_dataset.ix[:,0:520]))\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 520 \n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 128 \n",
    "n_hidden_3 = 64 \n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "total_batches = dataset.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
    "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "\n",
    "# --------------------- Encoder Variables --------------- #\n",
    "\n",
    "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
    "e_biases_h1 = bias_variable([n_hidden_1])\n",
    "\n",
    "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
    "e_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "e_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "# --------------------- Decoder Variables --------------- #\n",
    "\n",
    "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "d_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
    "d_biases_h2 = bias_variable([n_hidden_1])\n",
    "\n",
    "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
    "d_biases_h3 = bias_variable([n_input])\n",
    "\n",
    "# --------------------- DNN Variables ------------------ #\n",
    "\n",
    "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
    "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "#dnn_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "#dnn_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
    "dnn_biases_out = bias_variable([n_classes])\n",
    "\n",
    "# --------------------- CNN Variables ------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
    "    return l3\n",
    "    \n",
    "def decode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
    "    return l3\n",
    "\n",
    "def dnn(x):\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
    "    #dropout = tf.nn.dropout(l1, 0.5)\n",
    "    l2 = tf.nn.relu(tf.add(tf.matmul(l1,dnn_weights_h2),dnn_biases_h2))\n",
    "    #l3 = tf.nn.relu(tf.add(tf.matmul(l2,dnn_weights_h3),dnn_biases_h3))\n",
    "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode(X)\n",
    "decoded = decode(encoded) \n",
    "y_ = dnn(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of optimizer\n",
    "\n",
    "The following block of code is for the **AutoEncoder**. Calculate the result of **loss function** or **cost function** to evaluate autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
    "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
    "us_optimizer = tf.train.AdamOptimizer(learning_rate/10).minimize(us_cost_function)\n",
    "s_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(s_cost_function)\n",
    "# GradientDescent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy\n",
    "Following 2 lines is compare the results predicted by the network and the data of location in the testing set. `shape(y_) = [1111, 13]` while `shape(Y) = [1111]`as the there a 1111 testing data and the number of location lables is 13. The ocntents in `y_`, for each row, is the **possibility** that the point locates at the labled position. \n",
    "<br>\n",
    "<br>\n",
    "Function `tf.argmax(input, axis = 1)` will returns the **index** with the largest value across axes of a tensor but **not** the exact **value** of the maximum. If the shape of `input` is [12, 3], `axis = 1` will make the function takes index of maximum for each line, which means the shape of result will be [12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "Image take from: https://arxiv.org/pdf/1611.02049v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AE.png\">\n",
    "<img src=\"NN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.72804825329\n",
      "Epoch:  1  Loss:  0.708808635797\n",
      "Epoch:  2  Loss:  0.701586996689\n",
      "Epoch:  3  Loss:  0.698372220913\n",
      "Epoch:  4  Loss:  0.695565711761\n",
      "Epoch:  5  Loss:  0.693031316725\n",
      "Epoch:  6  Loss:  0.691171044374\n",
      "Epoch:  7  Loss:  0.689966472224\n",
      "Epoch:  8  Loss:  0.688681345756\n",
      "Epoch:  9  Loss:  0.687425040176\n",
      "Epoch:  10  Loss:  0.686673005378\n",
      "Epoch:  11  Loss:  0.68544389257\n",
      "Epoch:  12  Loss:  0.68514843986\n",
      "Epoch:  13  Loss:  0.684142158075\n",
      "Epoch:  14  Loss:  0.683717614506\n",
      "Epoch:  15  Loss:  0.682957470539\n",
      "Epoch:  16  Loss:  0.682622702466\n",
      "Epoch:  17  Loss:  0.682029398645\n",
      "Epoch:  18  Loss:  0.681458946536\n",
      "Epoch:  19  Loss:  0.681304433573\n",
      "Unsupervised pre-training finished...\n",
      "Epoch:  0  Loss:  3.66613967351  Training Accuracy:  0.694432 Validation Accuracy: 0.672131\n",
      "Epoch:  1  Loss:  1.08907778167  Training Accuracy:  0.828554 Validation Accuracy: 0.800298\n",
      "Epoch:  2  Loss:  0.640168520902  Training Accuracy:  0.884178 Validation Accuracy: 0.850969\n",
      "Epoch:  3  Loss:  0.451522333474  Training Accuracy:  0.913412 Validation Accuracy: 0.881769\n",
      "Epoch:  4  Loss:  0.345849328093  Training Accuracy:  0.933999 Validation Accuracy: 0.900149\n",
      "Epoch:  5  Loss:  0.281097530754  Training Accuracy:  0.951462 Validation Accuracy: 0.912072\n",
      "Epoch:  6  Loss:  0.238738078613  Training Accuracy:  0.960835 Validation Accuracy: 0.925484\n",
      "Epoch:  7  Loss:  0.209830331143  Training Accuracy:  0.968813 Validation Accuracy: 0.931942\n",
      "Epoch:  8  Loss:  0.187580790232  Training Accuracy:  0.975229 Validation Accuracy: 0.937407\n",
      "Epoch:  9  Loss:  0.169875908075  Training Accuracy:  0.979302 Validation Accuracy: 0.940387\n",
      "Epoch:  10  Loss:  0.15507169368  Training Accuracy:  0.983151 Validation Accuracy: 0.944362\n",
      "Epoch:  11  Loss:  0.143307913919  Training Accuracy:  0.986108 Validation Accuracy: 0.946846\n",
      "Epoch:  12  Loss:  0.133801997697  Training Accuracy:  0.987949 Validation Accuracy: 0.949329\n",
      "Epoch:  13  Loss:  0.126062104073  Training Accuracy:  0.989177 Validation Accuracy: 0.951316\n",
      "Epoch:  14  Loss:  0.119797734333  Training Accuracy:  0.990516 Validation Accuracy: 0.9538\n",
      "Epoch:  15  Loss:  0.114402410801  Training Accuracy:  0.991185 Validation Accuracy: 0.9538\n",
      "Epoch:  16  Loss:  0.10977274832  Training Accuracy:  0.991464 Validation Accuracy: 0.954794\n",
      "Epoch:  17  Loss:  0.105382723732  Training Accuracy:  0.991743 Validation Accuracy: 0.955291\n",
      "Epoch:  18  Loss:  0.101156658065  Training Accuracy:  0.99191 Validation Accuracy: 0.954297\n",
      "Epoch:  19  Loss:  0.0976750471757  Training Accuracy:  0.992245 Validation Accuracy: 0.954794\n",
      "Supervised training finished...\n",
      "\n",
      "Testing Accuracy: 0.774978\n",
      "[[  1.75509587e-04   1.54461631e-07   1.49858749e-07 ...,   5.27677685e-03\n",
      "    1.21783603e-04   3.20097251e-06]\n",
      " [  5.51308439e-08   5.14023029e-08   8.65034266e-10 ...,   1.60115553e-06\n",
      "    1.89180687e-01   8.10801268e-01]\n",
      " [  7.01965097e-10   5.21387600e-10   1.41609814e-12 ...,   3.38999107e-10\n",
      "    3.94426752e-06   9.99995708e-01]\n",
      " ..., \n",
      " [  9.79429483e-01   8.40489156e-06   1.72276100e-06 ...,   2.71666096e-07\n",
      "    5.36534799e-08   2.36381041e-07]\n",
      " [  9.58458602e-01   5.58551983e-06   8.33460376e-07 ...,   7.01760996e-07\n",
      "    5.71366492e-08   2.72692091e-07]\n",
      " [  9.99533892e-01   4.50227926e-05   4.01628512e-07 ...,   6.39722586e-08\n",
      "    1.21569412e-08   1.06442020e-08]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (features.shape[0] - batch_size)\n",
    "            batch_x = features[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
    "    print (\"Unsupervised pre-training finished...\")\n",
    "    \n",
    "    \n",
    "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
    "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
    "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
    "            \n",
    "    print (\"Supervised training finished...\")\n",
    "    \n",
    "\n",
    "    print (\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))\n",
    "    \n",
    "    print(session.run(y_, feed_dict={X: test_features, Y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Record\n",
    "\n",
    "2017/6/28 <br>\n",
    "- The output of the network is a [n_input, n_classes] tensor, each row represents the posibility of lables for each RSS.\n",
    "- Modify the number of training samples (reducing the scale of validation set). But the original rate 0.7 seems appropriate. Either higher or lower will reduce the accuracy.\n",
    "\n",
    "2017/7/5 <br>\n",
    "- Change the activation function in DNN classifier from `tf.nn.tanh` to `tf.nn.relu`. This just slightly improve the accuracy. If change all activation function even in autoencoder, the result even worse.\n",
    "\n",
    "2017/7/11 <br>\n",
    "- Try to add a hidden layer with 64 neural cells in DNN classifier. The result has no remarkable improvement. \n",
    "- Try to change the optimizer 'Adam' as same as the keras sample, the result even worse.\n",
    "\n",
    "2017/7/12 <br>\n",
    "- Try to use 'Adam' Optimizer with `learning_rate=1`, it is a little bit faster but, when `epoch = 20`, something wrong with training of DNN part. Once the epoch exceeds 13, it seems overfitting.\n",
    "\n",
    "\n",
    "2017/7/16 <br>\n",
    "Some suggestions from Jaehoo:\n",
    "- co-validation...\n",
    "- leaky relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
