{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders and Neural Network for Place recognition with WiFi fingerprints\n",
    "Implementation of algorithm discussed in <a href=\"https://arxiv.org/pdf/1611.02049v1.pdf\">Low-effort place recognition with WiFi fingerprints using Deep Learning </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\trainingData.csv\",header = 0)\n",
    "features = scale(np.asarray(dataset.ix[:,0:520]))\n",
    "# features = normalize(np.asarray(dataset.ix[:,0:520]))\n",
    "labels = np.asarray(dataset[\"BUILDINGID\"].map(str) + dataset[\"FLOOR\"].map(str))\n",
    "labels = np.asarray(pd.get_dummies(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing UJIndoorLoc training data set into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(features)) < 0.90\n",
    "train_x = features[train_val_split]\n",
    "train_y = labels[train_val_split]\n",
    "val_x = features[~train_val_split]\n",
    "val_y = labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using UJIndoorLoc validation data set as testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  \n",
      "c:\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\".\\\\UJIndoorLoc\\\\validationData.csv\",header = 0)\n",
    "test_features = scale(np.asarray(test_dataset.ix[:,0:520]))\n",
    "# test_features = normalize(np.asarray(test_dataset.ix[:,0:520]))\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 520 \n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 128 \n",
    "n_hidden_3 = 64 \n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "total_batches = dataset.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,n_input])\n",
    "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "\n",
    "# --------------------- Encoder Variables --------------- #\n",
    "\n",
    "e_weights_h1 = weight_variable([n_input, n_hidden_1])\n",
    "e_biases_h1 = bias_variable([n_hidden_1])\n",
    "\n",
    "e_weights_h2 = weight_variable([n_hidden_1, n_hidden_2])\n",
    "e_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "e_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "e_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "# --------------------- Decoder Variables --------------- #\n",
    "\n",
    "d_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "d_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "d_weights_h2 = weight_variable([n_hidden_2, n_hidden_1])\n",
    "d_biases_h2 = bias_variable([n_hidden_1])\n",
    "\n",
    "d_weights_h3 = weight_variable([n_hidden_1, n_input])\n",
    "d_biases_h3 = bias_variable([n_input])\n",
    "\n",
    "# --------------------- DNN Variables ------------------ #\n",
    "\n",
    "dnn_weights_h1 = weight_variable([n_hidden_3, n_hidden_2])\n",
    "dnn_biases_h1 = bias_variable([n_hidden_2])\n",
    "\n",
    "dnn_weights_h2 = weight_variable([n_hidden_2, n_hidden_2])\n",
    "dnn_biases_h2 = bias_variable([n_hidden_2])\n",
    "\n",
    "#dnn_weights_h3 = weight_variable([n_hidden_2, n_hidden_3])\n",
    "#dnn_biases_h3 = bias_variable([n_hidden_3])\n",
    "\n",
    "dnn_weights_out = weight_variable([n_hidden_2, n_classes])\n",
    "dnn_biases_out = bias_variable([n_classes])\n",
    "\n",
    "# --------------------- CNN Variables ------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,e_weights_h1),e_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,e_weights_h2),e_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,e_weights_h3),e_biases_h3))\n",
    "    return l3\n",
    "    \n",
    "def decode(x):\n",
    "    l1 = tf.nn.tanh(tf.add(tf.matmul(x,d_weights_h1),d_biases_h1))\n",
    "    l2 = tf.nn.tanh(tf.add(tf.matmul(l1,d_weights_h2),d_biases_h2))\n",
    "    l3 = tf.nn.tanh(tf.add(tf.matmul(l2,d_weights_h3),d_biases_h3))\n",
    "    return l3\n",
    "\n",
    "def dnn(x):\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(x,dnn_weights_h1),dnn_biases_h1))\n",
    "    dropout = tf.nn.dropout(l1, 0.5)\n",
    "    l2 = tf.nn.relu(tf.add(tf.matmul(dropout,dnn_weights_h2),dnn_biases_h2))\n",
    "    #l3 = tf.nn.relu(tf.add(tf.matmul(l2,dnn_weights_h3),dnn_biases_h3))\n",
    "    out = tf.nn.softmax(tf.add(tf.matmul(l2,dnn_weights_out),dnn_biases_out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded = encode(X)\n",
    "decoded = decode(encoded) \n",
    "y_ = dnn(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of optimizer\n",
    "\n",
    "The following block of code is for the **AutoEncoder**. Calculate the result of **loss function** or **cost function** to evaluate autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
    "s_cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
    "us_optimizer = tf.train.AdamOptimizer(learning_rate/10).minimize(us_cost_function)\n",
    "s_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(s_cost_function)\n",
    "# GradientDescent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy\n",
    "Following 2 lines is compare the results predicted by the network and the data of location in the testing set. `shape(y_) = [1111, 13]` while `shape(Y) = [1111]`as the there a 1111 testing data and the number of location lables is 13. The ocntents in `y_`, for each row, is the **possibility** that the point locates at the labled position. \n",
    "<br>\n",
    "<br>\n",
    "Function `tf.argmax(input, axis = 1)` will returns the **index** with the largest value across axes of a tensor but **not** the exact **value** of the maximum. If the shape of `input` is [12, 3], `axis = 1` will make the function takes index of maximum for each line, which means the shape of result will be [12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "Image take from: https://arxiv.org/pdf/1611.02049v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AE.png\">\n",
    "<img src=\"NN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.729673405863\n",
      "Epoch:  1  Loss:  0.709088220063\n",
      "Epoch:  2  Loss:  0.701908460516\n",
      "Epoch:  3  Loss:  0.697384272318\n",
      "Epoch:  4  Loss:  0.694975494965\n",
      "Epoch:  5  Loss:  0.693147233359\n",
      "Epoch:  6  Loss:  0.691330685004\n",
      "Epoch:  7  Loss:  0.68916690843\n",
      "Epoch:  8  Loss:  0.688502634096\n",
      "Epoch:  9  Loss:  0.686628009027\n",
      "Epoch:  10  Loss:  0.686124964255\n",
      "Epoch:  11  Loss:  0.68504933119\n",
      "Epoch:  12  Loss:  0.684594854206\n",
      "Epoch:  13  Loss:  0.68344018125\n",
      "Epoch:  14  Loss:  0.683213674837\n",
      "Epoch:  15  Loss:  0.682406320063\n",
      "Epoch:  16  Loss:  0.682460979154\n",
      "Epoch:  17  Loss:  0.681638495203\n",
      "Epoch:  18  Loss:  0.681252289051\n",
      "Epoch:  19  Loss:  0.680681935054\n",
      "Unsupervised pre-training finished...\n",
      "Epoch:  0  Loss:  5.29266438471  Training Accuracy:  0.673045 Validation Accuracy: 0.666156\n",
      "Epoch:  1  Loss:  1.82695287904  Training Accuracy:  0.788019 Validation Accuracy: 0.771822\n",
      "Epoch:  2  Loss:  1.17264775032  Training Accuracy:  0.850095 Validation Accuracy: 0.83563\n",
      "Epoch:  3  Loss:  0.87715333969  Training Accuracy:  0.892925 Validation Accuracy: 0.876978\n",
      "Epoch:  4  Loss:  0.688100004933  Training Accuracy:  0.916008 Validation Accuracy: 0.88974\n",
      "Epoch:  5  Loss:  0.570626050761  Training Accuracy:  0.936534 Validation Accuracy: 0.911179\n",
      "Epoch:  6  Loss:  0.481075680477  Training Accuracy:  0.94738 Validation Accuracy: 0.920368\n",
      "Epoch:  7  Loss:  0.406516135364  Training Accuracy:  0.957392 Validation Accuracy: 0.917815\n",
      "Epoch:  8  Loss:  0.367408139483  Training Accuracy:  0.964735 Validation Accuracy: 0.935681\n",
      "Epoch:  9  Loss:  0.328111393627  Training Accuracy:  0.968906 Validation Accuracy: 0.938744\n",
      "Epoch:  10  Loss:  0.315698780084  Training Accuracy:  0.970798 Validation Accuracy: 0.945891\n",
      "Epoch:  11  Loss:  0.282384075607  Training Accuracy:  0.97636 Validation Accuracy: 0.950485\n",
      "Epoch:  12  Loss:  0.26611572514  Training Accuracy:  0.978418 Validation Accuracy: 0.952527\n",
      "Epoch:  13  Loss:  0.256936377713  Training Accuracy:  0.982979 Validation Accuracy: 0.953548\n",
      "Epoch:  14  Loss:  0.225724888562  Training Accuracy:  0.983702 Validation Accuracy: 0.9561\n",
      "Epoch:  15  Loss:  0.217634484067  Training Accuracy:  0.985037 Validation Accuracy: 0.959673\n",
      "Epoch:  16  Loss:  0.192207099188  Training Accuracy:  0.986094 Validation Accuracy: 0.962736\n",
      "Epoch:  17  Loss:  0.187281160831  Training Accuracy:  0.987318 Validation Accuracy: 0.960184\n",
      "Epoch:  18  Loss:  0.18391583847  Training Accuracy:  0.987985 Validation Accuracy: 0.961205\n",
      "Epoch:  19  Loss:  0.165374130129  Training Accuracy:  0.989877 Validation Accuracy: 0.964267\n",
      "Supervised training finished...\n",
      "\n",
      "Testing Accuracy: 0.736274\n",
      "[[  8.32463629e-05   2.92155482e-06   6.08793425e-06 ...,   1.09571629e-05\n",
      "    9.40829224e-04   2.58796441e-04]\n",
      " [  5.07400788e-10   7.05090102e-08   1.09517923e-05 ...,   3.21920375e-08\n",
      "    1.22800982e-02   9.87707198e-01]\n",
      " [  6.66501034e-13   4.03723027e-10   4.44448324e-06 ...,   7.67688777e-12\n",
      "    2.83728554e-07   9.99995112e-01]\n",
      " ..., \n",
      " [  9.99855995e-01   1.11666985e-07   4.74348107e-15 ...,   7.14075650e-05\n",
      "    4.91417367e-12   2.97871378e-17]\n",
      " [  9.99598324e-01   2.22303308e-04   2.41732412e-10 ...,   1.57017595e-04\n",
      "    1.53759436e-10   2.66479633e-12]\n",
      " [  9.99977827e-01   1.96145047e-05   1.08389348e-11 ...,   1.63828574e-07\n",
      "    1.74164728e-11   7.72906129e-15]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (features.shape[0] - batch_size)\n",
    "            batch_x = features[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([us_optimizer, us_cost_function],feed_dict={X: batch_x})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs))\n",
    "    print (\"Unsupervised pre-training finished...\")\n",
    "    \n",
    "    \n",
    "    # ---------------- 2. Training NN - Supervised Learning ------------------ #\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_costs = np.empty(0)\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([s_optimizer, s_cost_function],feed_dict={X: batch_x, Y : batch_y})\n",
    "            epoch_costs = np.append(epoch_costs,c)\n",
    "        print (\"Epoch: \",epoch,\" Loss: \",np.mean(epoch_costs),\" Training Accuracy: \", \\\n",
    "            session.run(accuracy, feed_dict={X: train_x, Y: train_y}), \\\n",
    "            \"Validation Accuracy:\", session.run(accuracy, feed_dict={X: val_x, Y: val_y}))\n",
    "            \n",
    "    print (\"Supervised training finished...\")\n",
    "\n",
    "    print (\"\\nTesting Accuracy:\", session.run(accuracy, feed_dict={X: test_features, Y: test_labels}))\n",
    "    \n",
    "    print(session.run(y_, feed_dict={X: test_features, Y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Record\n",
    "\n",
    "2017/6/28 <br>\n",
    "- The output of the network is a [n_input, n_classes] tensor, each row represents the posibility of lables for each RSS.\n",
    "- Modify the number of training samples (reducing the scale of validation set). But the original rate 0.7 seems appropriate. Either higher or lower will reduce the accuracy.\n",
    "\n",
    "2017/7/5 <br>\n",
    "- Change the activation function in DNN classifier from `tf.nn.tanh` to `tf.nn.relu`. This just slightly improve the accuracy. If change all activation function even in autoencoder, the result even worse.\n",
    "\n",
    "2017/7/11 <br>\n",
    "- Try to add a hidden layer with 64 neural cells in DNN classifier. The result has no remarkable improvement. \n",
    "- Try to change the optimizer 'Adam' as same as the keras sample, the result even worse.\n",
    "\n",
    "2017/7/12 <br>\n",
    "- Try to use 'Adam' Optimizer with `learning_rate=1`, it is a little bit faster but, when `epoch = 20`, something wrong with training of DNN part. Once the epoch exceeds 13, it seems overfitting.\n",
    "\n",
    "\n",
    "2017/7/16 <br>\n",
    "Some suggestions from Jaehoo:\n",
    "- co-validation...\n",
    "- leaky relu"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
